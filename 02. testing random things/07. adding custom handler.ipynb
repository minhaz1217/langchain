{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"LLM New Token, token: {token}\")\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs) -> None:\n",
    "        print(f\"LLM start: {prompts}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs) -> None:\n",
    "        print(f\"LLM end: {response}\")\n",
    "\n",
    "\n",
    "myCustomHandler = MyCustomHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\HA HA\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Setting up the llm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "load_dotenv()\n",
    "huggingFaceApiToken = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=huggingFaceApiToken,\n",
    "    callbacks=[myCustomHandler],\n",
    ")\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM start: input_variables=['question'] template=\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
      "LLM end: generations=[[Generation(text=\" The FIFA World Cup is an international soccer competition. It is held every four years. The year 1994 is not a multiple of four, so it's not the usual year for a World Cup tournament. However, there was indeed a World Cup held in 1994. The United States won the tournament that year. So, the answer is: The United States won the FIFA World Cup in the year 1994.\")]] llm_output=None run=None\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The FIFA World Cup is an international soccer competition. It is held every four years. The year 1994 is not a multiple of four, so it's not the usual year for a World Cup tournament. However, there was indeed a World Cup held in 1994. The United States won the tournament that year. So, the answer is: The United States won the FIFA World Cup in the year 1994.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyComputer\\website\\python\\langchain\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM start: input_variables=['question'] template=\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
      "LLM end: generations=[[Generation(text='\\n\\nA common example of a white animal is the polar bear. Other animals with white fur include the Arctic fox, snow leopard, and swan. In the domestic animal world, we have the White Sheep, White Bengal Tiger, and the White Rabbit.')]] llm_output=None run=None\n"
     ]
    }
   ],
   "source": [
    "llmWithoutPrompt = llm(\"Give me an animal's name that is white.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA common example of a white animal is the polar bear. Other animals with white fur include the Arctic fox, snow leopard, and swan. In the domestic animal world, we have the White Sheep, White Bengal Tiger, and the White Rabbit.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmWithoutPrompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
