{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc00d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessResponse(models=[Model(model='deepseek-r1:1.5b', name='deepseek-r1:1.5b', digest='e0979632db5a88d1a53884cb2a941772d10ff5d055aabaa6801c4e36f3a6c2d7', expires_at=datetime.datetime(2025, 6, 8, 22, 16, 53, 799693, tzinfo=TzInfo(UTC)), size=2046867968, size_vram=2046867968, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# model = \"qwen2.5:0.5b\"\n",
    "# model = \"ollama3\"\n",
    "model = \"deepseek-r1:1.5b\"\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "client.ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "query = \"ONLY give me description of an ice cream that is pink color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4db13495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'score': {'title': 'Score', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['score', 'description'], 'title': 'RankingPrompt', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "request = type(\"object\", (object,), {})()\n",
    "request.query = query\n",
    "\n",
    "\n",
    "item = type(\"object\", (object,), {})()\n",
    "item.description = \"\"\"\n",
    "https://en.wikipedia.org/wiki/Superman_and_the_Mole_Men\t{\"@type\": \"Movie\", \"@id\": \"https://www.wikidata.org/wiki/Q1115677\", \"name\": \"Superman and the Mole Men\", \"datePublished\": 1951, \"director\": {\"@type\": \"Person\", \"name\": \"Lee Sholem\", \"@id\": \"http://www.wikidata.org/entity/Q6515055\"}, \"image\": \"https://commons.wikimedia.org/wiki/Special:FilePath/1951Superman002.jpg?width=800\", \"inLanguage\": \"English\", \"url\": \"https://en.wikipedia.org/wiki/Superman_and_the_Mole_Men\", \"description\": \"Superman and the Mole Men (titled onscreen as Superman and the Mole-Men, but all posters and advertising omit the hyphen) is a 1951 American independent black-and-white superhero film released by Lippert Pictures. Produced by Barney A. Sarecky and directed by Lee Sholem, it stars George Reeves as Superman and Phyllis Coates as Lois Lane. It is the first feature film based on any DC Comics character, and the first time George Reeves played the role of Superman. In the film's story, reporters Clark Kent and Lois Lane arrive in the small town of Silsby to witness the drilling of the world's deepest oil well. The drill, however, has penetrated the underground home of a race of small, bald humanoids who, out of curiosity, climb to the surface at night. They glow in the dark, which scares the local townspeople who form a mob intent on killing the creatures. Only Superman can intervene to prevent tragedy.\", \"actor\": [{\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q3565867\", \"name\": \"Walter Reed\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q3496795\", \"name\": \"Stanley Andrews\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q342537\", \"name\": \"George Reeves\", \"characterName\": \"Superman\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q448430\", \"name\": \"Jeff Corey\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q1307857\", \"name\": \"Frank Reicher\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q3177488\", \"name\": \"Jerry Maren\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q3038280\", \"name\": \"J. Farrell MacDonald\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q5004353\", \"name\": \"Byron Foulger\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q1311138\", \"name\": \"Phyllis Coates\", \"characterName\": \"Lois Lane\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q863063\", \"name\": \"Billy Curtis\"}, {\"@type\": \"Person\", \"@id\": \"http://www.wikidata.org/entity/Q15069891\", \"name\": \"Ray Walker\"}]}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DetectItemTypePrompt(BaseModel):\n",
    "    item_type: str\n",
    "\n",
    "\n",
    "class RankingPrompt(BaseModel):\n",
    "    score: int\n",
    "    description: str\n",
    "\n",
    "\n",
    "all_prompts = {\n",
    "    \"DetectItemTypePrompt\": {\n",
    "        \"prompt\": f\"\"\"What is the kind of item the query is likely seeking with this query: {request.query}\"\"\",\n",
    "        \"schema\": {\"item_type\": \"\"},\n",
    "        \"type_schema\": DetectItemTypePrompt,\n",
    "    },\n",
    "    \"RankingPrompt\": {\n",
    "        \"prompt\": f\"\"\"\n",
    "        Assign a score between 0 and 100 to the following item\n",
    "        based on how relevant it is to the user's question. Use your knowledge from other sources, about the item, to make a judgement. \n",
    "        If the score is above 50, provide a short description of the item highlighting the relevance to the user's question, without mentioning the user's question.\n",
    "        Provide an explanation of the relevance of the item to the user's question, without mentioning the user's question or the score or explicitly mentioning the term relevance.\n",
    "        If the score is below 75, in the description, include the reason why it is still relevant.\n",
    "        The user's question is: \\\"{request.query}\\\". The item's description in schema.org format is \\\"{item.description}\\\".\n",
    "    \"\"\",\n",
    "        \"schema\": {\n",
    "            \"score\": \"integer between 0 and 100\",\n",
    "            \"description\": \"short description of the item\",\n",
    "        },\n",
    "        \"type_schema\": RankingPrompt,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_prompt = all_prompts[\"RankingPrompt\"]\n",
    "\n",
    "prompt = current_prompt[\"prompt\"]\n",
    "schema = current_prompt[\"schema\"]\n",
    "system_prompt = f\"\"\"You are a helpful assistant that always provides responses in JSON format.\n",
    "Your response must be valid JSON that matches this schema: {json.dumps(schema)}\n",
    "Only output the JSON object, no additional text or explanation.\"\"\"\n",
    "type_schema = current_prompt[\"type_schema\"].model_json_schema()\n",
    "\n",
    "print(type_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='deepseek-r1:1.5b', created_at='2025-06-08T22:54:20.379009991Z', done=True, done_reason='stop', total_duration=3864011952, load_duration=296078743, prompt_eval_count=1007, prompt_eval_duration=12816976, eval_count=66, eval_duration=3546529680, message=Message(role='assistant', content='{\\n       \"score\": 75,\\n       \"description\": \"The item is a movie description that provides information about the movie\\'s title, release date, director, actors, genre, and other details. It is not directly relevant to the user\\'s question which is about an ice cream color in pink.\"\\n     }\\n    ', thinking=None, images=None, tool_calls=None))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# system_prompt = \"You are a helpful assistant\"\n",
    "# prompt = \"hi what is your name?\"\n",
    "# print(f\"Asking: {prompt}\")\n",
    "response = client.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    model=model,\n",
    "    options={\n",
    "        \"temperature\": .7,\n",
    "    },\n",
    "    format={'properties': {'score': {'title': 'Score', 'type': 'integer'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['score', 'description'], 'title': 'RankingPrompt', 'type': 'object'}\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
